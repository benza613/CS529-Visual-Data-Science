**Name:** Benito Alvares

**Summary:**

 - LL: With the recent advances in machine learning, there is increasing need for transparent and interpretable machine learning models. Interpretability is a crucial requirement for machine learning models in applications where human users are expected to sufficiently understand and trust them even without knowing the intricate details of their workings.

 - LL: Model induction can be used to infer an approximate and interpretable model from any ML model. It provides interpretability by treating any complex model as a black box without compromising the performance.

 - LL:  The simplest way to present a rule is to write it down as a logical expression, which is ubiquitous in programing languages. However, the authors found textual representations difficult to navigate when the length of the list of rules is too large. Hence the visual component of the interface is the matrix-based visualization of rules which lets users interpret and compare different rules very quickly.

**Discussion points:**

 - Q1: The visualization example images shown in the paper still seem to be a bit complex and display alot of information all at once, which doesn't seem to help unless there are some obvious indicators available in the dataset. 

 - Q2: Through this rule visulization, I can understand how well the classifier is performing, but it doesn't really tell me what I can change to improve the model performance