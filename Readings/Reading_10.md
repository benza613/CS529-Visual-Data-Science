**Name:** Benito Alvares

**Summary:**

 - LL: The initial choice of neural network architecture is still a significant barrier to access that limits the usability of neural networks. As the complexity of the problem grows it becomes harder and harder to manually design a neural network architecture. 

 - LL: The authors developed Sequential Neural Architecture Chips (SNACs) that are a space-efficient, adaptable visual encoding for neural networks that enabled comparison of many networks while still conveying shape and computation of those networks. This allowed feed forward neural networks to be represented in both visual analytics systems as well as static documents such as academic or industry white papers.

 - LL: REMAP is designed to be scalable inorder to facilitate human-in-the-loop-neural architecture search. By limiting the size of the architectures it discovers, the training times are reasonable so that the the user can remain engaged with the application

**Discussion points:**

 - Q1: In the event that one does have access to significant amount of computing power, wouldn't automated algorithms for neural architectures searching be able to provide better results as compared to REMAP?

 - Q2: While SNACs encoding seems to be doing well with one type of neural network like feed forward architecture, I wonder if it would be sufficient for concise visual representation in other types of networks like Recurrent or Hopfield networks where the task is related by time or memories.?